{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC305 Final Project: Guess Your Doodling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Repository Link: https://github.com/LiuJiang20/dsc_305_hand_scratch_recognition.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, convolutional neural network(CNN) has literally dominated the realm of image classification. Due to the explosion of computational power, human beings finally unleashed the blade of artificial neural network(ANN). As the descendent of ANN, CNN takes into account the fact that pixels close to each other are more related than pixels further apart, and it outperforms the previously used methods like logistic regression and support vector machine.   \n",
    "\n",
    "In this project, we explored the power of CNN using Quick, Draw! dataset provided by Google. Quick, Draw! is a web application. It prompts user with a word, the user draws an image according to that word, and then the computer guesses the word while the user is drawing the image. The Quick, Draw! dataset provides not only the final picture of users' hand drawings with the corresponding class labels, but also the sequence of strokes that the image is drawn by. Analyzing sequence of strokes would require recurrent neural network(RNN) which is complicated and hard to implement. Therefore, our project mainly focused on using CNN with pixels as inputs. \n",
    "\n",
    "The project consists of 2 parts: \n",
    "1. Build a CNN model that is able to classify hand-drawn images from 10 class labels using Keras, a high-level API for TensorFlow.  \n",
    "2. Deploy our model into a web application that mimic Quickdraw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all packages used in building our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.python.keras.backend as K\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our CNN model, let's prepare the dataset. The QuickDraw dataset are provided in multiple formats, for exmaple '.npy' and '.json'. In this project, we choose to download '.npy' files and convert them to '.png' image files that can be used directly when training our CNN model. First we pull all '.npy' files using the last commands in the block below. Before we pull files, we also want to craete a directory to store those files. That's what first four lines are doing. Notice that there is an exclamation point at the beginning of each line. This indicates that those are shell commands, not Python code. Here we assume we are at a Linux environment and have *gsutil* installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data\n",
    "!mkdir data\n",
    "!mkdir data/npy_data\n",
    "!mkdir data/png_data\n",
    "!gsutil -q -m cp 'gs://quickdraw_dataset/full/numpy_bitmap/*.npy' data/npy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have download our dataset, we can convert them into actual image files. As we are converting images, we will also split them into train, test and validation sets. So in our png_data directory, we create three subdirectories called \"train\", \"test\", \"val\". Test set and validation set will have the same structure as train set does except with less images. So, here we focus on how to construct our train set. Before we construct our train set, we have to decide how many different classes of hand-drawn images we want to use our model to predict, and for each class we will build a separate subdirectory under \"train\".  \n",
    "\n",
    "Originially, our goal is to include all 345 different classes of hand-drawn images. However, increase of class labels decreases the performance of our CNN model. If we include all classes, CNN model gives about 60% accuracy on the test set. It is not good enought to build a web application that can guess users' hand-drawings. Therefore, we decide that we will only use 10 classes of hand-drawn images (Ten classes: *hand, stop sign, book, clock, knife, pants, sweater, cloud, banana and car*. Classes are intentionally chosen to be as different in shape as possible to make it easier for the CNN to distiguish.) After we are settled for 10 classes to predict using our CNN model, we now can build a training set. Each '.npy' file contains all the images belongs to one specific class, and that is over 30,000 images. We will use first 5000 images for our training set. So for each class, we load all images into a huge numpy array from the corresponding '.npy' file, slice the numpy array for the first 5000 items, and store images in grayscale into a directory that corresponding to the type of the image. To store images, we will use *imwrite* in *opencv* package. We do the same thing for test and validation set. The validation set will take next 1000 images after the training set, and the test set will take next 2000 images after the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'data/npy_data'\n",
    "save_dir = 'data/png_data'\n",
    "shutil.rmtree(save_dir)\n",
    "os.mkdir(save_dir)\n",
    "os.mkdir(save_dir + '/' + 'train')\n",
    "os.mkdir(save_dir + '/' + 'test')\n",
    "os.mkdir(save_dir + '/' + 'val')\n",
    "files = os.listdir(file_dir)\n",
    "X_data = []\n",
    "y_data = []\n",
    "TRAIN_SIZE = 5000\n",
    "TEST_SIZE = 2000\n",
    "VAL_SIZE = 1000\n",
    "categories = {'hand.npy',\n",
    "'stop sign.npy',\n",
    "'book.npy',\n",
    "'clock.npy',\n",
    "'knife.npy',\n",
    "'pants.npy',\n",
    "'sweater.npy',\n",
    "'cloud.npy',\n",
    "'banana.npy',\n",
    "'car.npy'}\n",
    "for index, file in enumerate(files):\n",
    "    if file not in categories:\n",
    "        continue\n",
    "    train_images = np.load(file_dir+'/' + file)[:TRAIN_SIZE].copy()\n",
    "    test_images = np.load(file_dir+'/' + file)[TRAIN_SIZE:TRAIN_SIZE + TEST_SIZE].copy()\n",
    "    val_images = np.load(file_dir+'/' + file)[TRAIN_SIZE + TEST_SIZE : TRAIN_SIZE + TEST_SIZE + VAL_SIZE].copy()\n",
    "    # Here I force the GC to collect unused memory \n",
    "    # to ensure there is enough memory to load other pictures\n",
    "    gc.collect() \n",
    "    sub_dir = save_dir +'/'+'train'+ '/' + '_'.join(file.rstrip().split())\n",
    "    os.mkdir(sub_dir)\n",
    "    images = train_images.reshape(-1, 28, 28,1)\n",
    "    for index, image in enumerate(images):\n",
    "        cv2.imwrite(sub_dir + '/' + str(index)+'.png', image)\n",
    "    \n",
    "    sub_dir = save_dir +'/'+'test'+ '/' + '_'.join(file.rstrip().split())\n",
    "    os.mkdir(sub_dir)\n",
    "    images = test_images.reshape(-1, 28, 28,1)\n",
    "    for index, image in enumerate(images):\n",
    "        cv2.imwrite(sub_dir + '/' + str(index)+'.png', image)\n",
    "    sub_dir = save_dir +'/'+'val'+ '/' + '_'.join(file.rstrip().split())\n",
    "    os.mkdir(sub_dir)\n",
    "    images = val_images.reshape(-1, 28, 28,1)\n",
    "    for index, image in enumerate(images):\n",
    "        cv2.imwrite(sub_dir + '/' + str(index)+'.png', image)\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to leverage on Keras to prepare data stored on disk. To accerate training process, we want to keep GPU working all the time, that requires us to constantly feed data to GPU, so we need to store our data in memory. Howerver, typically memory is not large enough to hold all images, so we have to load images needed immediately to memory and remove images used from memory. Keras data generator handles all the detail for us and thus we won't worry about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n",
      "Found 20000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True\n",
    ")\n",
    "    \n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "TARGET_SIZE = (28,28)\n",
    "BATCH_SIZE = 1024\n",
    "CLASS_MODE = 'categorical'\n",
    "ROOT_DIR = 'data/png_data'\n",
    "COLOR_MODE = 'grayscale'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        ROOT_DIR + '/train',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        ROOT_DIR + '/val',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        ROOT_DIR + '/test',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Keras prepare our data, class labels are one-hot encoded. We also want to record the map, so that we know how to convert a number(if we consider class labels as integers from 1 to n) or an array of numbers(if we one-hot encode class labels) back to class labels. Here \"train_generator.class_indices\" is a dictionary. We will use a package called *pickle* to directly save the python object in binary into a file. Then we will want to map the CNN's predictions back to class labels. We can load the binary file to get our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "save_obj(train_generator.class_indices, 'ten_class_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create our CNN model. It contains 2 convolutional layers, each followed by a max-pooling layer. The results come from the second max-pooling layer is flattened and sent to a fully connnect network with 2 layers. For 2 convolutional layers and the first fully-connected layer, we use *relu* as the activation function. For our last layer, the output layer, we use *softmax* as our activation function, since it is a good fit for multi-class classification. We use *Categorical Cross Entropy* as our loss function and *Adam* as the optimizer. *Adam* the state-of-the-art optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(7 * 7 * 128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='adam',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our model for 50 epochs with validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "49/49 [==============================] - 8s 154ms/step - loss: 21.3438 - acc: 0.6686 - val_loss: 0.3903 - val_acc: 0.8880\n",
      "Epoch 2/50\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.3581 - acc: 0.8961 - val_loss: 0.2750 - val_acc: 0.9185\n",
      "Epoch 3/50\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 0.2712 - acc: 0.9196 - val_loss: 0.2372 - val_acc: 0.9303\n",
      "Epoch 4/50\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 0.2322 - acc: 0.9298 - val_loss: 0.2386 - val_acc: 0.9320\n",
      "Epoch 5/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.2093 - acc: 0.9357 - val_loss: 0.2223 - val_acc: 0.9354\n",
      "Epoch 6/50\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.1832 - acc: 0.9441 - val_loss: 0.2184 - val_acc: 0.9397\n",
      "Epoch 7/50\n",
      "49/49 [==============================] - 6s 114ms/step - loss: 0.1698 - acc: 0.9469 - val_loss: 0.2132 - val_acc: 0.9396\n",
      "Epoch 8/50\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 0.1547 - acc: 0.9521 - val_loss: 0.2107 - val_acc: 0.9389\n",
      "Epoch 9/50\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 0.1408 - acc: 0.9556 - val_loss: 0.2120 - val_acc: 0.9402\n",
      "Epoch 10/50\n",
      "49/49 [==============================] - 6s 114ms/step - loss: 0.1298 - acc: 0.9584 - val_loss: 0.2073 - val_acc: 0.9416\n",
      "Epoch 11/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.1188 - acc: 0.9621 - val_loss: 0.2146 - val_acc: 0.9408\n",
      "Epoch 12/50\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 0.1093 - acc: 0.9640 - val_loss: 0.2206 - val_acc: 0.9416\n",
      "Epoch 13/50\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 0.1084 - acc: 0.9646 - val_loss: 0.2057 - val_acc: 0.9439\n",
      "Epoch 14/50\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.0937 - acc: 0.9688 - val_loss: 0.2125 - val_acc: 0.9437\n",
      "Epoch 15/50\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 0.0925 - acc: 0.9707 - val_loss: 0.2191 - val_acc: 0.9420\n",
      "Epoch 16/50\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 0.0852 - acc: 0.9715 - val_loss: 0.2186 - val_acc: 0.9437\n",
      "Epoch 17/50\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 0.0789 - acc: 0.9738 - val_loss: 0.2275 - val_acc: 0.9424\n",
      "Epoch 18/50\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.0739 - acc: 0.9762 - val_loss: 0.2189 - val_acc: 0.9447\n",
      "Epoch 19/50\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 0.0699 - acc: 0.9769 - val_loss: 0.2329 - val_acc: 0.9430\n",
      "Epoch 20/50\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 0.0673 - acc: 0.9784 - val_loss: 0.2304 - val_acc: 0.9452\n",
      "Epoch 21/50\n",
      "49/49 [==============================] - 5s 109ms/step - loss: 0.0638 - acc: 0.9794 - val_loss: 0.2271 - val_acc: 0.9456\n",
      "Epoch 22/50\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.0638 - acc: 0.9795 - val_loss: 0.2337 - val_acc: 0.9444\n",
      "Epoch 23/50\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.0593 - acc: 0.9804 - val_loss: 0.2463 - val_acc: 0.9438\n",
      "Epoch 24/50\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.0580 - acc: 0.9810 - val_loss: 0.2325 - val_acc: 0.9438\n",
      "Epoch 25/50\n",
      "49/49 [==============================] - 6s 121ms/step - loss: 0.0571 - acc: 0.9815 - val_loss: 0.2417 - val_acc: 0.9447\n",
      "Epoch 26/50\n",
      "49/49 [==============================] - 6s 114ms/step - loss: 0.0526 - acc: 0.9828 - val_loss: 0.2480 - val_acc: 0.9418\n",
      "Epoch 27/50\n",
      "49/49 [==============================] - 6s 120ms/step - loss: 0.0522 - acc: 0.9832 - val_loss: 0.2430 - val_acc: 0.9427\n",
      "Epoch 28/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.0500 - acc: 0.9835 - val_loss: 0.2489 - val_acc: 0.9435\n",
      "Epoch 29/50\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 0.0487 - acc: 0.9846 - val_loss: 0.2568 - val_acc: 0.9439\n",
      "Epoch 30/50\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 0.0478 - acc: 0.9847 - val_loss: 0.2529 - val_acc: 0.9465\n",
      "Epoch 31/50\n",
      "49/49 [==============================] - 6s 118ms/step - loss: 0.0482 - acc: 0.9845 - val_loss: 0.2482 - val_acc: 0.9442\n",
      "Epoch 32/50\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 0.0478 - acc: 0.9854 - val_loss: 0.2561 - val_acc: 0.9434\n",
      "Epoch 33/50\n",
      "49/49 [==============================] - 6s 114ms/step - loss: 0.0457 - acc: 0.9855 - val_loss: 0.2474 - val_acc: 0.9441\n",
      "Epoch 34/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.0451 - acc: 0.9849 - val_loss: 0.2538 - val_acc: 0.9441\n",
      "Epoch 35/50\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 0.0432 - acc: 0.9862 - val_loss: 0.2614 - val_acc: 0.9453\n",
      "Epoch 36/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.0389 - acc: 0.9875 - val_loss: 0.2464 - val_acc: 0.9449\n",
      "Epoch 37/50\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 0.0410 - acc: 0.9868 - val_loss: 0.2760 - val_acc: 0.9430\n",
      "Epoch 38/50\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 0.0435 - acc: 0.9870 - val_loss: 0.2668 - val_acc: 0.9465\n",
      "Epoch 39/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.0386 - acc: 0.9875 - val_loss: 0.2662 - val_acc: 0.9453\n",
      "Epoch 40/50\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 0.0412 - acc: 0.9866 - val_loss: 0.2653 - val_acc: 0.9449\n",
      "Epoch 41/50\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 0.0390 - acc: 0.9875 - val_loss: 0.2666 - val_acc: 0.9448\n",
      "Epoch 42/50\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 0.0375 - acc: 0.9880 - val_loss: 0.2714 - val_acc: 0.9477\n",
      "Epoch 43/50\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 0.0394 - acc: 0.9872 - val_loss: 0.2724 - val_acc: 0.9442\n",
      "Epoch 44/50\n",
      "49/49 [==============================] - 5s 111ms/step - loss: 0.0362 - acc: 0.9887 - val_loss: 0.2517 - val_acc: 0.9473\n",
      "Epoch 45/50\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 0.0335 - acc: 0.9893 - val_loss: 0.2837 - val_acc: 0.9431\n",
      "Epoch 46/50\n",
      "49/49 [==============================] - 5s 110ms/step - loss: 0.0352 - acc: 0.9889 - val_loss: 0.2786 - val_acc: 0.9446\n",
      "Epoch 47/50\n",
      "49/49 [==============================] - 6s 113ms/step - loss: 0.0357 - acc: 0.9886 - val_loss: 0.2803 - val_acc: 0.9439\n",
      "Epoch 48/50\n",
      "49/49 [==============================] - 6s 120ms/step - loss: 0.0350 - acc: 0.9892 - val_loss: 0.2703 - val_acc: 0.9453\n",
      "Epoch 49/50\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 0.0332 - acc: 0.9893 - val_loss: 0.2768 - val_acc: 0.9463\n",
      "Epoch 50/50\n",
      "49/49 [==============================] - 5s 111ms/step - loss: 0.0342 - acc: 0.9889 - val_loss: 0.2751 - val_acc: 0.9474\n"
     ]
    }
   ],
   "source": [
    "history= model.fit_generator(train_generator, epochs=50,\n",
    "                                      validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is done, we evaluate our CNN model. It shows 90% accuracy on test set, which is pretty exciting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(test_generator, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.26713376119732857\n",
      "acc: 0.94625\n"
     ]
    }
   ],
   "source": [
    "print('loss:', loss)\n",
    "print('acc:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sava our model so that it can be exported to javascript files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ten_class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a model for hand-drawn images classification, deploying that can provide readers a simple intuitive way to play with the model and see how well it performs. Thus, we decide to apply our model to a web application that mimics QuickDraw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build the frame of our web application:\n",
    "1. A list of words on the top allows the user to pick one and draw picture based on that. The *Change* button allows the user to change a list of new words, but it doesn't work at the current stage, because we have only built one model with 10 class labels. \n",
    "2. The canvas on the left provides the user a place to draw. Users can click the *Guess* button and the *Retry* button to perform further actions.\n",
    "3. The robot with a dialog box on the right can give users feedback, and users can click the *Correct!* button and the *Wrong!* button to respond. For each round, the robot has 3 chances to guess."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![webApplication.png](webApplication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we implement all functionlities. The following steps show how our program works:\n",
    "1. Users pick a word and start drawing.\n",
    "2. Load the model to the javascript file.\n",
    "3. Get content on the canvas after users finish drawing and click the *Guess* button. Convert the RGB image to grayscale image, and then store the image data as a array of pixels.\n",
    "4. Transfer the size of users' hand-drawn image from 512x512 to 28x28 by using maxPool, so that the image can fit to our model.\n",
    "5. Feed the model with the image, get prediction, and give users feedback. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flowchart.png](flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples show how a image looks like after saving as a grayscale image and transfering its size:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![examples.png](examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, our group built a CNN model that can classify hand-drawn images, and we deployed that model to a web application. In the future, we want to refine our model by training more class labels. As I mentioned in the introduction part, increasing the number of class labels causes poor model performance. We may improve that by tuning some hyperparameters. Another way to enrich our model and web application is that we can let the model learn from users' inputs, both successful cases and failed cases. That can further realize the idea of artificial intelligence. Also, because the QuickDraw dataset stores the sequence of strokes that each image is drawn by, we can try to build a RNN model based on that. In that case, our model can make prediction while the user is drawing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
