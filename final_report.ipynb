{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC305 Final Project: Guess Your Doodling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, convolutional neural network(CNN) has literally dominates the realm of image classification. It outperforms previously used method like logistic regression and support vector machine.  ... ... . \n",
    "In this project, we will explore the power of CNN using the quickdraw dataset provided by Google. Quickdraw is an web application. It prompts user with a word, the user will draw an image for the word and the computer will guess the word will the user is drawing the image. The quickdraw dataset not only provided the final picture of users' hand-schatched images and the corresponding class labels, but also the sequence of strokes that the image is drawn by. Analyzing sequence of drawing would require recurrent neural network which is specialized for sequence data. In our project, we mainly focused on using CNN taking pixels as input to predict the class label of the hand-scatched image. Our final project consists of 2 parts. In part one, we would build a CNN that is able to classify hand-drawn images from 10 classes using Keras, a highe-level API for tensorflow.In part 2, we will deploy our model into a web application that mimic Quickdraw provided by Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all packages used in building our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.python.keras.backend as K\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our CNN model, let's prepare our dataset. The quickdraw dataset are provided in multiple formats, for exmaple '.npy' and '.json'. In this project, we choose to download '.npy' files and convert them to '.png' image files that can be used directly when training our CNN model. First we pull all '.npy' files using the last commands in the block below. Before we pull files, we also want to craete the directory to store those files. That's what first four lines are doing. Notice that there is an exclamation point at the beginning of each line. This indicates that those are shell commands, not Python code. Here we assume we are at a Linux environment and have *gsutil* installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data\n",
    "!mkdir data\n",
    "!mkdir data/npy_data\n",
    "!mkdir data/png_data\n",
    "!gsutil -q -m cp 'gs://quickdraw_dataset/full/numpy_bitmap/*.npy' data/npy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have download our dataset, we can convert them into actual image files. As we are converting images, we will also split them into train, test and validation sets. So in our png_data directory, we create three subdirectories called \"train\", \"test\", \"val\". \"Test\" and \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'data/npy_data'\n",
    "save_dir = 'data/png_data'\n",
    "shutil.rmtree(save_dir)\n",
    "os.mkdir(save_dir)\n",
    "os.mkdir(save_dir + '/' + 'train')\n",
    "os.mkdir(save_dir + '/' + 'test')\n",
    "os.mkdir(save_dir + '/' + 'val')\n",
    "files = os.listdir(file_dir)\n",
    "X_data = []\n",
    "y_data = []\n",
    "TRAIN_SIZE = 5000\n",
    "TEST_SIZE = 2000\n",
    "VAL_SIZE = 1000\n",
    "for index, file in enumerate(files):\n",
    "    if index > 10:\n",
    "        break\n",
    "    train_images = np.load(file_dir+'/' + file)[:TRAIN_SIZE].copy()\n",
    "    test_images = np.load(file_dir+'/' + file)[TRAIN_SIZE:TRAIN_SIZE + TEST_SIZE].copy()\n",
    "    val_images = np.load(file_dir+'/' + file)[TRAIN_SIZE + TEST_SIZE : TRAIN_SIZE + TEST_SIZE + VAL_SIZE].copy()\n",
    "    # Here I force the GC to collect unused memory \n",
    "    # to ensure there is enought memory to load other pictures\n",
    "    gc.collect() \n",
    "    sub_dir = save_dir +'/'+'train'+ '/' + '_'.join(file.rstrip().split())\n",
    "    os.mkdir(sub_dir)\n",
    "    images = train_images.reshape(-1, 28, 28,1)\n",
    "    for index, image in enumerate(images):\n",
    "        cv2.imwrite(sub_dir + '/' + str(index)+'.png', image)\n",
    "    \n",
    "    sub_dir = save_dir +'/'+'test'+ '/' + '_'.join(file.rstrip().split())\n",
    "    os.mkdir(sub_dir)\n",
    "    images = test_images.reshape(-1, 28, 28,1)\n",
    "    for index, image in enumerate(images):\n",
    "        cv2.imwrite(sub_dir + '/' + str(index)+'.png', image)\n",
    "    sub_dir = save_dir +'/'+'val'+ '/' + '_'.join(file.rstrip().split())\n",
    "    os.mkdir(sub_dir)\n",
    "    images = val_images.reshape(-1, 28, 28,1)\n",
    "    for index, image in enumerate(images):\n",
    "        cv2.imwrite(sub_dir + '/' + str(index)+'.png', image)\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55000 images belonging to 11 classes.\n",
      "Found 11000 images belonging to 11 classes.\n",
      "Found 22000 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True\n",
    ")\n",
    "    \n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "TARGET_SIZE = (28,28)\n",
    "BATCH_SIZE = 1024\n",
    "CLASS_MODE = 'categorical'\n",
    "ROOT_DIR = 'data/png_data'\n",
    "COLOR_MODE = 'grayscale'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        ROOT_DIR + '/train',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        ROOT_DIR + '/val',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        ROOT_DIR + '/test',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "save_obj(train_generator.class_indices, 'eleven_class_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(7 * 7 * 128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(11, activation='softmax')])\n",
    "model.compile(optimizer='adam',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 [==============================] - 8s 148ms/step - loss: 18.6460 - acc: 0.4809 - val_loss: 0.7751 - val_acc: 0.7655\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 7s 121ms/step - loss: 0.7157 - acc: 0.7798 - val_loss: 0.5313 - val_acc: 0.8419\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 7s 130ms/step - loss: 0.5639 - acc: 0.8291 - val_loss: 0.4759 - val_acc: 0.8575\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.4970 - acc: 0.8494 - val_loss: 0.4331 - val_acc: 0.8738\n",
      "Epoch 5/50\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 0.4507 - acc: 0.8625 - val_loss: 0.4283 - val_acc: 0.8746\n",
      "Epoch 6/50\n",
      "54/54 [==============================] - 7s 130ms/step - loss: 0.4175 - acc: 0.8726 - val_loss: 0.4179 - val_acc: 0.8749\n",
      "Epoch 7/50\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 0.3921 - acc: 0.8809 - val_loss: 0.4034 - val_acc: 0.8825\n",
      "Epoch 8/50\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 0.3660 - acc: 0.8868 - val_loss: 0.4033 - val_acc: 0.8814\n",
      "Epoch 9/50\n",
      "54/54 [==============================] - 6s 114ms/step - loss: 0.3525 - acc: 0.8914 - val_loss: 0.3841 - val_acc: 0.8889\n",
      "Epoch 10/50\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 0.3150 - acc: 0.9020 - val_loss: 0.3738 - val_acc: 0.8929\n",
      "Epoch 11/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.3042 - acc: 0.9058 - val_loss: 0.3803 - val_acc: 0.8885\n",
      "Epoch 12/50\n",
      "54/54 [==============================] - 6s 112ms/step - loss: 0.2876 - acc: 0.9103 - val_loss: 0.3709 - val_acc: 0.8945\n",
      "Epoch 13/50\n",
      "54/54 [==============================] - 6s 110ms/step - loss: 0.2706 - acc: 0.9146 - val_loss: 0.3639 - val_acc: 0.8975\n",
      "Epoch 14/50\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 0.2588 - acc: 0.9187 - val_loss: 0.3818 - val_acc: 0.8908\n",
      "Epoch 15/50\n",
      "54/54 [==============================] - 6s 118ms/step - loss: 0.2452 - acc: 0.9229 - val_loss: 0.3728 - val_acc: 0.8946\n",
      "Epoch 16/50\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 0.2272 - acc: 0.9263 - val_loss: 0.3669 - val_acc: 0.8956\n",
      "Epoch 17/50\n",
      "54/54 [==============================] - 6s 112ms/step - loss: 0.2187 - acc: 0.9302 - val_loss: 0.3733 - val_acc: 0.8974\n",
      "Epoch 18/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.2093 - acc: 0.9335 - val_loss: 0.3823 - val_acc: 0.8937\n",
      "Epoch 19/50\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 0.1971 - acc: 0.9373 - val_loss: 0.3659 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 0.1908 - acc: 0.9375 - val_loss: 0.3841 - val_acc: 0.8964\n",
      "Epoch 21/50\n",
      "54/54 [==============================] - 7s 130ms/step - loss: 0.1813 - acc: 0.9418 - val_loss: 0.3906 - val_acc: 0.8981\n",
      "Epoch 22/50\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 0.1753 - acc: 0.9443 - val_loss: 0.3968 - val_acc: 0.8954\n",
      "Epoch 23/50\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 0.1677 - acc: 0.9463 - val_loss: 0.3974 - val_acc: 0.8976\n",
      "Epoch 24/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.1598 - acc: 0.9481 - val_loss: 0.3876 - val_acc: 0.8992\n",
      "Epoch 25/50\n",
      "54/54 [==============================] - 6s 111ms/step - loss: 0.1519 - acc: 0.9507 - val_loss: 0.4042 - val_acc: 0.8955\n",
      "Epoch 26/50\n",
      "54/54 [==============================] - 7s 124ms/step - loss: 0.1477 - acc: 0.9516 - val_loss: 0.3951 - val_acc: 0.8998\n",
      "Epoch 27/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.1413 - acc: 0.9542 - val_loss: 0.4123 - val_acc: 0.8976\n",
      "Epoch 28/50\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 0.1399 - acc: 0.9546 - val_loss: 0.4069 - val_acc: 0.8995\n",
      "Epoch 29/50\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 0.1330 - acc: 0.9569 - val_loss: 0.4164 - val_acc: 0.8945\n",
      "Epoch 30/50\n",
      "54/54 [==============================] - 6s 114ms/step - loss: 0.1324 - acc: 0.9576 - val_loss: 0.4093 - val_acc: 0.9002\n",
      "Epoch 31/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.1255 - acc: 0.9590 - val_loss: 0.4111 - val_acc: 0.8999\n",
      "Epoch 32/50\n",
      "54/54 [==============================] - 6s 112ms/step - loss: 0.1247 - acc: 0.9591 - val_loss: 0.4195 - val_acc: 0.9006\n",
      "Epoch 33/50\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 0.1218 - acc: 0.9599 - val_loss: 0.4078 - val_acc: 0.9005\n",
      "Epoch 34/50\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 0.1129 - acc: 0.9636 - val_loss: 0.4290 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 0.1125 - acc: 0.9641 - val_loss: 0.4176 - val_acc: 0.9005\n",
      "Epoch 36/50\n",
      "54/54 [==============================] - 6s 112ms/step - loss: 0.1091 - acc: 0.9650 - val_loss: 0.4487 - val_acc: 0.8958\n",
      "Epoch 37/50\n",
      "54/54 [==============================] - 7s 121ms/step - loss: 0.1042 - acc: 0.9661 - val_loss: 0.4286 - val_acc: 0.8986\n",
      "Epoch 38/50\n",
      "54/54 [==============================] - 7s 123ms/step - loss: 0.0996 - acc: 0.9686 - val_loss: 0.4379 - val_acc: 0.8988\n",
      "Epoch 39/50\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 0.1034 - acc: 0.9668 - val_loss: 0.4402 - val_acc: 0.9002\n",
      "Epoch 40/50\n",
      "54/54 [==============================] - 7s 121ms/step - loss: 0.1009 - acc: 0.9670 - val_loss: 0.4482 - val_acc: 0.8978\n",
      "Epoch 41/50\n",
      "54/54 [==============================] - 6s 119ms/step - loss: 0.0998 - acc: 0.9681 - val_loss: 0.4320 - val_acc: 0.9019\n",
      "Epoch 42/50\n",
      "54/54 [==============================] - 6s 119ms/step - loss: 0.0954 - acc: 0.9694 - val_loss: 0.4528 - val_acc: 0.8996\n",
      "Epoch 43/50\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 0.0947 - acc: 0.9698 - val_loss: 0.4446 - val_acc: 0.8998\n",
      "Epoch 44/50\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 0.0936 - acc: 0.9708 - val_loss: 0.4443 - val_acc: 0.8989\n",
      "Epoch 45/50\n",
      "54/54 [==============================] - 6s 119ms/step - loss: 0.0905 - acc: 0.9717 - val_loss: 0.4548 - val_acc: 0.9001\n",
      "Epoch 46/50\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 0.0922 - acc: 0.9701 - val_loss: 0.4465 - val_acc: 0.9002\n",
      "Epoch 47/50\n",
      "54/54 [==============================] - 6s 119ms/step - loss: 0.0861 - acc: 0.9727 - val_loss: 0.4569 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 0.0867 - acc: 0.9727 - val_loss: 0.4611 - val_acc: 0.9029\n",
      "Epoch 49/50\n",
      "54/54 [==============================] - 7s 129ms/step - loss: 0.0804 - acc: 0.9744 - val_loss: 0.4357 - val_acc: 0.9020\n",
      "Epoch 50/50\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 0.0855 - acc: 0.9729 - val_loss: 0.4562 - val_acc: 0.8994\n"
     ]
    }
   ],
   "source": [
    "history= model.fit_generator(train_generator, epochs=50,\n",
    "                                      validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(test_generator, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.45300136371092364\n",
      "acc: 0.9013636\n"
     ]
    }
   ],
   "source": [
    "print('loss:', loss)\n",
    "print('acc:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('eleven_class.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
